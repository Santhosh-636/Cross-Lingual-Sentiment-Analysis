import sys
import os
import importlib
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import pandas as pd

def try_scrape(module_name, candidates):
    """Import scraper module and try candidate functions. If none found,
    try any callable in the module starting with 'scrape'. Normalize
    output to a list of dicts with at least 'headline' and 'language'."""
    lang_map = {
        'times_of_india': 'en',
        'ndtv': 'en',
        'vijaya_karnataka': 'kn',
        'dinamani': 'ta'
    }

    try:
        mod = importlib.import_module(f"scrapers.{module_name}")
    except Exception:
        return None

    # Try explicit candidate names first
    for fn in candidates:
        if hasattr(mod, fn):
            try:
                out = getattr(mod, fn)()
                return normalize_scraper_output(out, lang_map.get(module_name, 'en'))
            except Exception:
                continue

    # Fallback: find any callable starting with 'scrape'
    for name in dir(mod):
        if name.startswith('scrape') and callable(getattr(mod, name)):
            try:
                out = getattr(mod, name)()
                return normalize_scraper_output(out, lang_map.get(module_name, 'en'))
            except Exception:
                continue

    return None


def normalize_scraper_output(out, default_lang='en'):
    """Convert various scraper return types into a list of dicts.
    Accepts list[str], list[dict], single dict, or None."""
    if not out:
        return None

    results = []
    if isinstance(out, dict):
        out = [out]

    if isinstance(out, (list, tuple)):
        for item in out:
            if isinstance(item, str):
                results.append({'headline': item, 'language': default_lang})
            elif isinstance(item, dict):
                # Ensure headline key exists
                headline = item.get('headline') or item.get('text') or item.get('title')
                if not headline:
                    continue
                row = {'headline': headline, 'language': item.get('language', default_lang)}
                if 'link' in item:
                    row['link'] = item['link']
                results.append(row)
    return results

# Attempt to scrape; fall back to sample data if scraper missing/failing
headlines = {}
sources = [
    ('times_of_india', ['scrape_toi', 'scrape_times_of_india']),
    ('ndtv', ['scrape_ndtv', 'scrape_ndtv_headlines']),
    ('vijaya_karnataka', ['scrape_vijaya', 'scrape_vijaya_karnataka']),
    ('dinamani', ['scrape_dinamani', 'scrape_dinamani_headlines'])
]

# default sample fallback (kept for offline/demo)
sample_fallback = {
    'times_of_india': [
        {'headline': 'Accident on highway kills 5', 'language': 'en'} ,
        {'headline': 'India wins cricket match', 'language': 'en'}
    ],
    'ndtv': [
        {'headline': 'Highway accident claims lives', 'language': 'en'},
        {'headline': 'India cricket victory celebrated', 'language': 'en'}
    ],
    'vijaya_karnataka': [
        {'headline': '‡≤π‡≥Ü‡≤¶‡≥ç‡≤¶‡≤æ‡≤∞‡≤ø‡≤Ø‡≤≤‡≥ç‡≤≤‡≤ø ‡≤Ö‡≤™‡≤ò‡≤æ‡≤§', 'language': 'kn'},
        {'headline': '‡≤≠‡≤æ‡≤∞‡≤§ ‡≤ï‡≥ç‡≤∞‡≤ø‡≤ï‡≥Ü‡≤ü‡≥ç ‡≤ó‡≥Ü‡≤≤‡≥ç‡≤™‡≥Å', 'language': 'kn'}
    ],
    'dinamani': [
        {'headline': '‡Æ™‡Ææ‡Æ§‡Øà‡Æ™‡Øã‡Æï‡Øç‡Æï‡ØÅ ‡Æµ‡Æø‡Æ™‡Æ§‡Øç‡Æ§‡ØÅ', 'language': 'ta'},
        {'headline': '‡Æá‡Æ®‡Øç‡Æ§‡Æø‡ÆØ‡Ææ ‡Æï‡Æø‡Æ∞‡Æø‡Æï‡Øç‡Æï‡ØÜ‡Æü‡Øç ‡Æµ‡ØÜ‡Æ±‡Øç‡Æ±‡Æø', 'language': 'ta'}
    ]
}

raw_outputs = {}
for name, candidates in sources:
    out = try_scrape(name, candidates)
    if not out:
        print(f"[!] Scraper '{name}' returned no results; using fallback sample")
        out = sample_fallback.get(name, [])
        used_fallback = True
    else:
        print(f"[+] Scraper '{name}' returned {len(out)} items")
        used_fallback = False
    # store raw output and the fallback flag for inspection
    raw_outputs[name] = {'items': out, 'used_fallback': used_fallback}
    headlines[name] = out

# save raw outputs for debugging
import json
reports_dir = os.path.join(os.path.dirname(__file__), '..', 'reports')
os.makedirs(reports_dir, exist_ok=True)
raw_path = os.path.join(reports_dir, 'raw_headlines.json')
with open(raw_path, 'w', encoding='utf-8') as rf:
    json.dump(raw_outputs, rf, ensure_ascii=False, indent=2)
print(f"Saved raw scraper outputs to: {os.path.abspath(raw_path)}")

# Build dataframe (expect scrapers to return list of dicts with 'text' or 'headline')
rows = []
for source, items in headlines.items():
    for i, it in enumerate(items):
        text = it.get('text') or it.get('headline') or str(it)
        score = it.get('score', 0.0)
        label = it.get('label', 'NEUTRAL')
        lang = it.get('language', it.get('lang', 'en'))
        rows.append({'source': source, 'headline': text, 'language': lang, 'sentiment_score': score, 'sentiment_label': label})

df = pd.DataFrame(rows)
from nlp.translators import translate_text, detect_language
from sentiment.analyzer import analyze_headlines

# If headlines are not yet analyzed, translate non-English and analyze sentiment
enriched = {}
for src, items in headlines.items():
    # items are list of dicts with 'headline' and 'language'
    to_analyze = []
    for it in items:
        h = dict(it)
        # ensure language detection if missing
        lang = h.get('language') or detect_language(h.get('headline', ''))
        h['language'] = lang
        if lang != 'en':
            # translate to English for sentiment analysis
            try:
                h['translated_headline'] = translate_text(h['headline'], 'en')
            except Exception:
                h['translated_headline'] = h['headline']
        else:
            h['translated_headline'] = h['headline']
        to_analyze.append(h)
    # analyze sentiment on translated text
    # prepare input with translated headline in 'headline' key for analyzer
    analyzer_input = [{'headline': t['translated_headline'], 'language': t['language']} for t in to_analyze]
    analyzed = analyze_headlines(analyzer_input)
    # merge back sentiment into original items
    merged = []
    for orig, a in zip(to_analyze, analyzed):
        merged_item = dict(orig)
        merged_item.update({'sentiment_score': a.get('sentiment_score', 0.0), 'sentiment_label': a.get('sentiment_label', 'NEUTRAL'), 'confidence': a.get('confidence', 0.0)})
        merged.append(merged_item)
    enriched[src] = merged

# rebuild dataframe from enriched data
rows = []
for source, items in enriched.items():
    for r in items:
        rows.append({'source': source, 'headline': r.get('headline'), 'translated_headline': r.get('translated_headline'), 'language': r.get('language'), 'sentiment_score': r.get('sentiment_score', 0.0), 'sentiment_label': r.get('sentiment_label', 'NEUTRAL'), 'confidence': r.get('confidence', 0.0), 'link': r.get('link')})

df = pd.DataFrame(rows)

# Terminal-only formatted output (no web page, no browser)
print("\n" + "="*80)
print("CROSS-LINGUAL SENTIMENT ANALYSIS ‚Äî TERMINAL REPORT")
print("Generated:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
print("="*80 + "\n")

print("HEADLINES\n" + "-"*80)
for src in df['source'].unique():
    print(f"\n{src.replace('_',' ').upper()}")
    print("-"*40)
    sub = df[df['source'] == src]
    for _, r in sub.iterrows():
        emoji = "üòî" if r['sentiment_label'] == 'NEGATIVE' else "üòä" if r['sentiment_label'] == 'POSITIVE' else "üòê"
        print(f"{emoji} [{r['language'].upper()}] {r['headline']}")
        print(f"    Score: {r['sentiment_score']:+.2f}  Label: {r['sentiment_label']}")
    print()

print("\n" + "="*80)
print("MEDIA BIAS SUMMARY\n" + "-"*80)
for src in sorted(df['source'].unique()):
    sub = df[df['source'] == src]
    avg = sub['sentiment_score'].mean()
    pos = (sub['sentiment_label'] == 'POSITIVE').sum()
    neg = (sub['sentiment_label'] == 'NEGATIVE').sum()
    print(f"{src.replace('_',' ').title():25} Avg: {avg:+.3f}   +:{pos}  -:{neg}")

print("\n" + "="*80)
print("KEY METRICS")
print("- Total headlines:", len(df))
print("- Sources:", df['source'].nunique())
print("- Languages:", ', '.join(sorted(df['language'].unique())))
print("- Overall average sentiment:", f"{df['sentiment_score'].mean():+.3f}")
print("="*80 + "\n")

# Optional: save a plain text summary file
out_dir = os.path.join(os.path.dirname(__file__), '..', 'reports')
os.makedirs(out_dir, exist_ok=True)
summary_path = os.path.join(out_dir, 'terminal_report.txt')
with open(summary_path, 'w', encoding='utf-8') as f:
    f.write("Cross-Lingual Sentiment Analysis ‚Äî Terminal Report\n")
    f.write("Generated: " + datetime.now().isoformat() + "\n\n")
    f.write(df.to_string(index=False))
    f.write("\n\nMedia bias summary\n")
    for src in sorted(df['source'].unique()):
        sub = df[df['source'] == src]
        avg = sub['sentiment_score'].mean()
        pos = (sub['sentiment_label'] == 'POSITIVE').sum()
        neg = (sub['sentiment_label'] == 'NEGATIVE').sum()
        f.write(f"{src}: Avg {avg:+.3f}  +:{pos}  -:{neg}\n")

print(f"Plain-text summary saved to: {os.path.abspath(summary_path)}")